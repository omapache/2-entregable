# Proyectos con Hugging Face Transformers

Este repositorio contiene tres proyectos que exploran diferentes aplicaciones de modelos de Transformers y su implementaci칩n tanto de manera local como mediante APIs de Hugging Face.

## Configuraci칩n del Token

Antes de ejecutar los proyectos, aseg칰rate de configurar correctamente tu token de Hugging Face. Para utilizar modelos a trav칠s de la API, es necesario:

1. Crear un token de acceso en tu cuenta de Hugging Face.
   - Ve a [Tu cuenta en Hugging Face](https://huggingface.co/settings/tokens) y genera un nuevo token con los permisos necesarios.
2. Agregar permisos espec칤ficos para el modelo que deseas usar.
   - Durante la configuraci칩n del token, aseg칰rate de incluir los modelos requeridos en la lista de permisos.

---

## Requisitos para la Ejecuci칩n

- **Python 3.7 o superior**
- **Librer칤as necesarias**:
  - `transformers`
  - `openai`
  - `torch`

Instala las dependencias necesarias con:

```bash
pip install transformers torch sentencepiece
```

---

## Proyecto 1: An치lisis de Sentimientos

Este proyecto utiliza un modelo de an치lisis de sentimientos que se ejecuta localmente para clasificar texto en categor칤as como positivo, negativo o neutral.

### Detalles del Modelo Usado
- **Modelo**: nlptown/bert-base-multilingual-uncased-sentiment
- **Caracter칤sticas**:
  - Multiling칲e, con soporte para espa침ol.
  - Clasificaci칩n de sentimientos en cinco categor칤as.

### Implementaci칩n
1. Descarga del modelo utilizando la librer칤a `transformers`.
2. Entrada del texto por parte del usuario.
3. Salida con la categor칤a del sentimiento y su puntuaci칩n.

### Ejemplo de Ejecuci칩n

```bash
python entregable.py
```
Cuando se solicite, ingresa el texto:
```text
Ingresa tu texto: "Me encanta este producto, es fant치stico."

Salida: ES HAPPY (0.95)
```

---

## Proyecto 2: Aplicaci칩n de Preguntas y Respuestas (LLM)

Este proyecto utiliza una API de Hugging Face para implementar una aplicaci칩n donde el usuario puede realizar preguntas y obtener respuestas del modelo.

### Detalles del Modelo Usado
- **Modelo**: mistralai/Mistral-7B-Instruct-v0.2
- **Caracter칤sticas**:
  - Modelo de lenguaje grande (7 mil millones de par치metros).
  - Optimizado para tareas basadas en instrucciones.
  - Soporte multiling칲e.

### Implementaci칩n
1. Carga del modelo a trav칠s de la API de Hugging Face.
2. Generaci칩n de respuestas con temperatura 0.7 para balancear creatividad y coherencia.

### Ejemplo de Ejecuci칩n

```bash
python entregable1.py
```
Cuando se solicite, ingresa tu pregunta:
```text
Ingresa tu token: "hf_ASUJDHAJDHASJDH"

Ingresa tu pregunta: "쮺u치l es la capital de Francia?"
Respuesta: "The capital city of Spain is Madrid. Madrid is a vibrant and major cultural and political center of Spain. Its rich history and art, including the Prado Museum and the Royal Palace, attract millions of tourists every year. Madrid is also known for its numerous parks, wide tree-lined boulevards, and lively atmosphere, making it a popular destination for both domestic and international travelers."
```

---

## Proyecto 3: Generaci칩n de Im치genes a partir de Texto generado por una pregunta

Este proyecto utiliza una API de Hugging Face para implementar una aplicaci칩n que permite al usuario generar texto y crear im치genes basadas en ese texto.

### Modelos Implementados

1. **Generaci칩n de Texto**
   - **Modelo**: mistralai/Mistral-7B-Instruct-v0.2
   - **Tarea**: Responder preguntas y generar texto a partir de instrucciones.

2. **Generaci칩n de Im치genes**
   - **Modelo**: black-forest-labs/FLUX.1-dev
   - **Tarea**: Crear im치genes a partir de descripciones textuales.

### Implementaci칩n
1. Creaci칩n de una clase que cargue din치micamente diferentes modelos.
2. Uso de m칠todos espec칤ficos para alternar entre tareas.
3. Entrada y salida configuradas seg칰n la tarea seleccionada.

### Ejemplo de Ejecuci칩n

Cuando se solicite, ingresa tu pregunta:
```text
Ingresa tu token: "hf_ASUJDHAJDHASJDH"

Ingresa tu pregunta: 쮺u치l es la capital de Espa침a?

Salida: "The capital city of Spain is Madrid. Madrid is a vibrant and major cultural and political center of Spain. Its rich history and art, including the Prado Museum and the Royal Palace, attract millions of tourists every year. Madrid is also known for its numerous parks, wide tree-lined boulevards, and lively atmosphere, making it a popular destination for both domestic and international travelers."

crea la imagen llamada : "imagen_generada" en base a ese texto generado*
```
---

## Ejecuci칩n Local y Aprendizajes sobre Transformers

### 쯈u칠 son los Transformers?
Lo que llamamos Transformers son arquitecturas de redes neuronales dise침adas para procesar secuencias, como texto o im치genes, con alta eficiencia. Destacan por:
- **Atenci칩n bidireccional**: Contexto m치s rico para tareas de lenguaje.
- **Procesamiento en paralelo**: Mayor velocidad frente a las RNNs tradicionales.

### Ventajas
- **Independencia**: No se necesita conexi칩n a APIs externas.
- **Privacidad**: Los datos se procesan localmente.
- **Menor latencia**: Respuestas m치s r치pidas.

### Desventajas
- **Recursos computacionales**: Algunos modelos son exigentes.
- **Configuraci칩n de hardware**: Se recomienda el uso de GPU para modelos grandes.


##
por Owen 游붛
![alt text](imagenFinal.jpg)
